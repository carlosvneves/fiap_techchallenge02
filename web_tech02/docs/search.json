[
  {
    "objectID": "models.html#médias-móveis-do-ibovespa-para-7-15-e-30-dias",
    "href": "models.html#médias-móveis-do-ibovespa-para-7-15-e-30-dias",
    "title": "1 Modelagem e previsão",
    "section": "1.1 Médias-móveis do Ibovespa para 7, 15 e 30 dias",
    "text": "1.1 Médias-móveis do Ibovespa para 7, 15 e 30 dias\n\nMédias-móveis são técnicas de suavização de séries temporais e, no máximo, permitem que seja realizada a previsão para o período imediatamente posterior.\n\n\n\n\n\n\n\n\nFigure 1: Série temporal com dados observados e após eliminação de anomalias\n\n\n\n\n\n\n\n\n\n\nEstatísticas das médias-móveis e da série original\n\n\n\ndate\nclose\nrolling_avg_7d\nrolling_avg_15d\nrolling_avg_30d\n\n\n\n\n\nMin. :2010-01-04\nMin. : 37.50\nMin. : 37.74\nMin. : 38.12\nMin. : 39.11\n\n\n\n1st Qu.:2013-07-12\n1st Qu.: 56.28\n1st Qu.: 56.27\n1st Qu.: 56.25\n1st Qu.: 56.23\n\n\n\nMedian :2017-01-17\nMedian : 68.15\nMedian : 68.10\nMedian : 68.06\nMedian : 67.89\n\n\n\nMean :2017-01-17\nMean : 78.11\nMean : 78.11\nMean : 78.11\nMean : 78.11\n\n\n\n3rd Qu.:2020-07-25\n3rd Qu.:103.22\n3rd Qu.:103.34\n3rd Qu.:103.37\n3rd Qu.:103.29\n\n\n\nMax. :2024-01-31\nMax. :133.89\nMax. :133.00\nMax. :132.78\nMax. :132.06\n\n\n\n\n\n\n\n\nTable 1: Estatísticas das médias-móveis e da série original.\n\n\n\n\nCom o aumento da janela temporal, podemos verificar que houve uma redução no valor da mediana, assim como no valor máximo da série.\n\n\n\n\n\n\nMedidas de acurácia da média-móvel para 7, 15 e 30 dias, previsão para 1 dia e respectivo MAPE\n\n\n\nmae\nmape\nmase\nrmse\nfct\nfct_mape\n\n\n\n\nrolling_avg_7d\n0.4373373\n0.5794936\n0.8092715\n0.5992426\n128.0787\n0.3131462\n\n\nrolling_avg_15d\n0.6743177\n0.9039829\n1.2477923\n0.8792734\n128.2599\n0.1721072\n\n\nrolling_avg_30d\n0.9331605\n1.2623652\n1.7267685\n1.1851084\n127.9105\n0.4440086\n\n\n\n\n\n\n\n\nTable 2: Medidas de acurácia da média-móvel para 7, 15 e 30 dias, previsão para 1 dia e respectivo MAPE\n\n\n\n\nOs resultados acima mostram que o modelo pode ser considerado acurado para realizar a previsão para o dia seguinte, pois a sua natureza não é de previsão, mas sim de suavização. Neste caso, o valor de fechamento do dia 01/02/2024 real foi de 128,481, e o MAPE foi calculado em relação a este valor."
  },
  {
    "objectID": "models.html#divisão-entre-teste-e-treino",
    "href": "models.html#divisão-entre-teste-e-treino",
    "title": "1 Modelagem e previsão",
    "section": "1.2 Divisão entre teste e treino",
    "text": "1.2 Divisão entre teste e treino\nO primeiro passo para o treinamento de modelos mais sofisticados é a divisão dos dados entre conjuntos de teste e treinamento. Neste caso, o conjunto de teste corresponderá a 20% do conjunto original.\n\n\n\n\n\n\n\n\nFigure 2: Ibovespa fechamento - Conjuntos de Teste e Treino"
  },
  {
    "objectID": "models.html#criação-e-ajuste-de-múltiplos-modelos",
    "href": "models.html#criação-e-ajuste-de-múltiplos-modelos",
    "title": "1 Modelagem e previsão",
    "section": "1.3 Criação e ajuste de múltiplos modelos",
    "text": "1.3 Criação e ajuste de múltiplos modelos\nOs modelos a seguir serão ajustados utilizando a biblioteca timetk. Esta biblioteca permite o ajuste de diversos modelos de séries temporais dentro de um framework coerente, criando uma camada de abstração em relação a várias outras bibliotecas tradicionais na modelagem de séries temporais, como a biblioteca forecast. A biblioteca também permite a integração com fluxos de trabalho de machine-learning como o H2O AutoML.\n\n1.3.1 AUTO-ARIMA\nA função arima_reg() é uma maneira de gerar uma especificação de um modelo ARIMA e retorna o melhor modelo de acordo com os valores AIC, AICc ou BIC. A função realiza uma busca pelos possíveis modelos dentro das restrições de ordem fornecidas.\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\nmodel_fit_arima_no_boost &lt;- arima_reg(\n        # ARIMA args\n        seasonal_period = 253, #\"auto\" ,\n        non_seasonal_ar = 5,\n        non_seasonal_differences = 1,\n        non_seasonal_ma = 1,\n        seasonal_ar     = 5,\n        seasonal_differences = 5,\n        seasonal_ma     = 5,\n) |&gt; \n    set_engine(engine = \"auto_arima\") |&gt; \n    fit(close ~ date, data = training(splits))\n\n\n\n\n1.3.2 AUTO-ARIMA (Boost)\nA função arima_boost() é uma forma de gerar a especificação de um modelo de séries temporais que utiliza boosting para aprimorar erros de modelagem (resíduos) em regressores exógenos. Ela funciona tanto com ARIMA “automatizado” (auto.arima), quanto com ARIMA padrão (arima). Os principais algoritmos são:\n\nARIMA Automatizado + Erros XGBoost (engine = auto_arima_xgboost, padrão)\nARIMA + Erros XGBoost (engine = arima_xgboost)\n\nIremos usar o auto_arima_xgboost para que o algoritmo escolha automaticamente os parâmetros que melhor ajustam o modelo.\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\n\nmodel_fit_arima_boost &lt;- arima_boost(\n    # ARIMA args\n        mode = \"regression\",\n        seasonal_period = \"auto\" ,\n        non_seasonal_ar = 2,\n        non_seasonal_differences = 2,\n        non_seasonal_ma = 2,\n        seasonal_ar     = 1,\n        seasonal_differences = 1,\n        seasonal_ma     = 1,\n        \n\n    # XGBoost Args\n    trees = 10,\n    tree_depth = 25, \n    learn_rate = 0.015,\n    mtry = 1,\n    sample_size = 0.5,\n    stop_iter = 2) |&gt; \n    set_engine(engine = \"auto_arima_xgboost\") |&gt; \n    fit(close ~ date + as.numeric(date) \n        + factor(lubridate::month(date, label=T))\n        + factor(lubridate::week(date)), \n        data = training(splits))\n\n\nfrequency = 7 observations per 1 week\n\n\nWarning: `colsample_bynode = 1` with `counts = TRUE` will only sample a single column.\n                            Set `counts = FALSE` to use a proportion (100% of columns).\n\n\nCode\nmodel_fit_arima_boost$fit$models$model_1$model\n\n\n$phi\nnumeric(0)\n\n$theta\nnumeric(0)\n\n$Delta\n[1] 1\n\n$Z\n[1] 1 1\n\n$a\n[1]  -0.727 105.747\n\n$P\n              [,1]          [,2]\n[1,]  0.000000e+00 -7.420876e-23\n[2,] -7.420876e-23  7.420876e-23\n\n$T\n     [,1] [,2]\n[1,]    0    0\n[2,]    1    1\n\n$V\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    0\n\n$h\n[1] 0\n\n$Pn\n     [,1]         [,2]\n[1,]    1 0.000000e+00\n[2,]    0 7.420876e-23\n\n\n\n\n1.3.3 Exponential Smoothing\nEm seguida, crie um modelo Error-Trend-Season (ETS) usando um modelo state-space de suavização exponencial. Isso é feito com exp_smoothing().\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\nmodel_fit_ets &lt;- exp_smoothing(trend = \"additive\", \n                               season = \"auto\", \n                               smooth_level = 0.3, \n                               smooth_trend = 0.15) |&gt; \n    set_engine(engine = \"ets\")  |&gt; \n    fit(close ~ date, data = training(splits))\n\n\nfrequency = 7 observations per 1 week\n\n\n\n\n1.3.4 Prophet\nO modelo seguinte é o Prophet utilizando prophet_reg(). Neste modelo foram inseridas informações sobre os lockdowns ocorridos em 2020 e 2021 no Estado de São Paulo. Vale ressaltar que os lockdowns foram bastante simplificados, tendo em vista que não foi possível homogeneizar as informações.\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\n\nlockdown1 &lt;- tibble(holiday = 'lockdown',  \n                    ds = as.Date(c('2020-02-21')),\n                    lower_window = 0, \n                    upper_window = '2020-02-27') \n\nlockdown2 &lt;- tibble(holiday = 'lockdown',  \n                    ds = as.Date(c('2020-03-24')),\n                    lower_window = 0, \n                    upper_window = '2021-08-16') \n\n\nlockdowns &lt;- bind_rows(lockdown1, lockdown2)\n\nmodel_fit_prophet &lt;- prophet_reg(seasonality_weekly = T, \n                                 seasonality_daily = T,\n                                 seasonality_yearly = \"auto\",\n                                 prior_scale_seasonality = 45,\n                                 prior_scale_changepoints = 0.6,\n                                 changepoint_range = 0.9,\n                                 changepoint_num = 35\n                                 # modelar covid19\n                                 ) |&gt; \n    set_engine(engine = \"prophet\", holidays = lockdowns) |&gt; \n    fit(close ~ date, data = training(splits))\n\n\n\n\n1.3.5 Prophet (Boost)\nEste modelo é do tipo Prophet utilizando prophet_boost(). É um modelo similar ao anterior, porém utiliza o boosting como uma forma de gerar a especificação de um modelo de séries temporais para aprimorar erros de modelagem (resíduos). Neste modelo não foram inseridas informações sobre os lockdowns.\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\nmodel_fit_prophet_boost &lt;- prophet_boost(mode=\"regression\",\n                                         seasonality_weekly = T, \n                                 seasonality_daily = T,\n                                 seasonality_yearly = \"auto\",\n                                 prior_scale_seasonality = 45,\n                                 prior_scale_changepoints = 0.65,\n                                 changepoint_range = 0.9,\n                                 changepoint_num = 35,\n                                 # XGBoost Args\n                                  trees = 10,\n                                  tree_depth = 30, \n                                  learn_rate = 0.008,\n                                  mtry = 1,\n                                  sample_size = 0.5,\n                                  stop_iter = 5,\n                                 \n                                   ) |&gt; \n    set_engine(engine = \"prophet_xgboost\", counts = F) |&gt; \n    fit(close ~ date + as.numeric(date) \n        + factor(lubridate::month(date, label=T))\n        + factor(lubridate::week(date)), \n        data = training(splits))\n    #fit(close ~ date , data = training(splits))\n\n\n\n\n1.3.6 Regressão Linear (Parsip)\nPodemos modelar uma regressão linear de série temporal (TSLM) usando o algoritmo linear_reg(). Os seguintes regressores derivados de date são usados:\n\nTendência: modelada usando as.numeric(date);\nSazonalidade: modelado usando month(date) e week(date).\n\n\n\nCode\n#|echo: false\n#|warning: false\n#|cache: true\n#|code-fold: show\n\nmodel_fit_lm &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") |&gt; \n    fit(close ~ as.numeric(date)   \n        + factor(lubridate::month(date, label=T), ordered = FALSE)\n        + factor(lubridate::week(date)), \n        data = training(splits))\n\n\n\n\n1.3.7 GLMNET\nO modelo glmnet é um método de regressão que combina as técnicas de Lasso e Ridge para seleção de variáveis e regularização. Ele funciona através da minimização de uma função de custo que inclui o erro quadrático médio e um termo de penalização que controla a complexidade do modelo.\nAbaixo é utilizado um fluxo de trabalho para padronizar o pré-processamento dos recursos fornecidos ao modelo de aprendizado de máquina.\n\n\n# A tibble: 4,369 × 29\n   date       close date_index.num date_year date_year.iso date_half\n   &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n 1 2010-01-04  70.0     1262563200      2010          2010         1\n 2 2010-01-05  70.2     1262649600      2010          2010         1\n 3 2010-01-06  70.7     1262736000      2010          2010         1\n 4 2010-01-07  70.5     1262822400      2010          2010         1\n 5 2010-01-08  70.3     1262908800      2010          2010         1\n 6 2010-01-09  70.3     1262995200      2010          2010         1\n 7 2010-01-10  70.4     1263081600      2010          2010         1\n 8 2010-01-11  70.4     1263168000      2010          2010         1\n 9 2010-01-12  70.1     1263254400      2010          2010         1\n10 2010-01-13  70.4     1263340800      2010          2010         1\n# ℹ 4,359 more rows\n# ℹ 23 more variables: date_quarter &lt;int&gt;, date_month &lt;int&gt;,\n#   date_month.xts &lt;int&gt;, date_month.lbl &lt;ord&gt;, date_day &lt;int&gt;,\n#   date_hour &lt;int&gt;, date_minute &lt;int&gt;, date_second &lt;int&gt;, date_hour12 &lt;int&gt;,\n#   date_am.pm &lt;int&gt;, date_wday &lt;int&gt;, date_wday.xts &lt;int&gt;,\n#   date_wday.lbl &lt;ord&gt;, date_mday &lt;int&gt;, date_qday &lt;int&gt;, date_yday &lt;int&gt;,\n#   date_mweek &lt;int&gt;, date_week &lt;int&gt;, date_week.iso &lt;int&gt;, date_week2 &lt;int&gt;, …\n\n\n# A tibble: 4,369 × 65\n   close date_index.num date_year date_half date_quarter date_month date_day\n   &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;        &lt;int&gt;      &lt;int&gt;    &lt;dbl&gt;\n 1  70.0          -1.73      2010         1            1          1   -1.33 \n 2  70.2          -1.73      2010         1            1          1   -1.22 \n 3  70.7          -1.73      2010         1            1          1   -1.10 \n 4  70.5          -1.73      2010         1            1          1   -0.991\n 5  70.3          -1.73      2010         1            1          1   -0.877\n 6  70.3          -1.73      2010         1            1          1   -0.764\n 7  70.4          -1.73      2010         1            1          1   -0.650\n 8  70.4          -1.73      2010         1            1          1   -0.536\n 9  70.1          -1.73      2010         1            1          1   -0.422\n10  70.4          -1.72      2010         1            1          1   -0.309\n# ℹ 4,359 more rows\n# ℹ 58 more variables: date_second &lt;int&gt;, date_wday &lt;dbl&gt;, date_mday &lt;int&gt;,\n#   date_qday &lt;dbl&gt;, date_yday &lt;int&gt;, date_mweek &lt;int&gt;, date_week &lt;int&gt;,\n#   date_week2 &lt;int&gt;, date_week3 &lt;int&gt;, date_week4 &lt;int&gt;, date_mday7 &lt;int&gt;,\n#   date_sin63.25_K1 &lt;dbl&gt;, date_cos63.25_K1 &lt;dbl&gt;, date_sin63.25_K2 &lt;dbl&gt;,\n#   date_cos63.25_K2 &lt;dbl&gt;, date_sin63.25_K3 &lt;dbl&gt;, date_cos63.25_K3 &lt;dbl&gt;,\n#   date_sin63.25_K4 &lt;dbl&gt;, date_cos63.25_K4 &lt;dbl&gt;, date_sin63.25_K5 &lt;dbl&gt;, …"
  },
  {
    "objectID": "models.html#sumário-dos-modelos",
    "href": "models.html#sumário-dos-modelos",
    "title": "1 Modelagem e previsão",
    "section": "1.4 Sumário dos modelos",
    "text": "1.4 Sumário dos modelos\nA lista de modelos adotados está a seguir:\n\n\nCode\nmodels_tbl &lt;- modeltime_table(\n    model_fit_arima_no_boost,\n    model_fit_arima_boost,\n    model_fit_ets,\n    model_fit_prophet,\n    model_fit_prophet_boost,\n    model_fit_lm,\n    wflw_fit_glmnet\n\n)\n\nkable(tibble(Modelos = models_tbl$.model_desc), \n      caption = \"List de modelos\",) |&gt; \n      kable_styling(full_width = FALSE, \n                position = \"center\") |&gt; \n  column_spec(1, bold = TRUE) \n\n\n\n\n\n\nList de modelos\n\n\nModelos\n\n\n\n\nARIMA(0,1,0)\n\n\nARIMA(0,1,0) W/ XGBOOST ERRORS\n\n\nETS(M,AD,N)\n\n\nPROPHET\n\n\nPROPHET W/ XGBOOST ERRORS\n\n\nLM\n\n\nGLMNET\n\n\n\n\n\n\n\n\nTable 3: Lista de modelos"
  },
  {
    "objectID": "models.html#ajuste-dos-modelos-aos-dados-de-teste",
    "href": "models.html#ajuste-dos-modelos-aos-dados-de-teste",
    "title": "1 Modelagem e previsão",
    "section": "1.5 Ajuste dos modelos aos dados de teste",
    "text": "1.5 Ajuste dos modelos aos dados de teste\n\n\nCode\ncalibration_tbl &lt;- models_tbl |&gt; \n    modeltime_calibrate(new_data = testing(splits))\n\n\nkable(tibble(Modelo = calibration_tbl$.model_desc,\n             Tipo = calibration_tbl$.type), \n      caption = \"Modelos e ajuste aos dados de treino\") |&gt; \n      kable_styling(full_width = FALSE, \n                position = \"center\") |&gt; \n  column_spec(1, bold = TRUE) \n\n\n\n\n\n\nModelos e ajuste aos dados de treino\n\n\nModelo\nTipo\n\n\n\n\nARIMA(0,1,0)\nTest\n\n\nARIMA(0,1,0) W/ XGBOOST ERRORS\nTest\n\n\nETS(M,AD,N)\nTest\n\n\nPROPHET\nTest\n\n\nPROPHET W/ XGBOOST ERRORS\nTest\n\n\nLM\nTest\n\n\nGLMNET\nTest\n\n\n\n\n\n\n\n\nTable 4: Tabela de modelos e ajuste aos dados de teste"
  },
  {
    "objectID": "models.html#testando-as-previsões-e-avaliação-de-acurácia",
    "href": "models.html#testando-as-previsões-e-avaliação-de-acurácia",
    "title": "1 Modelagem e previsão",
    "section": "1.6 Testando as previsões e avaliação de acurácia",
    "text": "1.6 Testando as previsões e avaliação de acurácia\n\n1.6.1 Visualização das previsões em relação aos dados de teste\nA Figure 3 permite comparar os diversos modelos aos dados de teste. Os modelos Prophet, Prophet Boost, LM e GLMNET são, aparentemente, aqueles que conseguem melhor captar o padrão sazonal e de tendência da série original. Os outros modelos apresentam um comportamento mais flat.\n\n\n\n\n\n\n\n\nFigure 3: Previsões em relação aos dados de teste\n\n\n\n\n\n\n1.6.2 Medição de acurácia das previsões em relação aos dados de teste\nNa Table 5 podemos ver a acurácia dos modelos em relação aos dados de teste.\n\n\n\n\n\n\n\n\n\n\n\nMétricas de acurácia - previsão x dados de teste\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nARIMA(0,1,0)\nTest\n8.55\n7.27\n12.31\n7.67\n10.84\nNA\n\n\n2\nARIMA(0,1,0) W/ XGBOOST ERRORS\nTest\n8.26\n7.03\n11.90\n7.40\n10.54\n0.01\n\n\n3\nETS(M,AD,N)\nTest\n9.15\n7.78\n13.17\n8.24\n11.46\n0.01\n\n\n4\nPROPHET\nTest\n10.93\n10.10\n15.74\n9.45\n12.75\n0.04\n\n\n5\nPROPHET W/ XGBOOST ERRORS\nTest\n10.70\n9.89\n15.41\n9.25\n12.58\n0.01\n\n\n6\nLM\nTest\n7.76\n6.67\n11.17\n6.96\n9.25\n0.35\n\n\n7\nGLMNET\nTest\n8.06\n6.94\n11.61\n7.24\n9.52\n0.35\n\n\n\n\n\n\n\n\nTable 5: Métricas de acurácia - previsão x dados de teste\n\n\n\n\nTomando o MAPE como métrica, podemos verificar que os modelos LM e GLMNET são os que apresentam menor erro. Chama a atenção o fato de que, apesar de não conseguirem captar os padrões de tendência e sazonalidade, os modelos ARIMA(com e sem boosting) e ETS conseguem apresentar um MAPE menor que dos modelos do tipo PROPHET.\n\n\n1.6.3 Refit para o dataset completo e previsão fora-da-amostra\nUma vez satisfeitos com nosso conjunto de modelos, podemos aplicar modeltime_refit() no conjunto de dados completo e fazer previsões para fora da amostra, incluindo os intervalos de confiança.\nComo o conjunto de dados disponível fora da amostra (conjuntos de teste e treino) é de 20 dias, este é o número de períodos adiante para os quais faremos as previsões. Estas previsões poderão então ser comparadas aos índices diários de fechamento efetivamente realizados.\n\n\n\n\n\n\n\n\nFigure 4: Previsões para 20 dias fora da amostra"
  },
  {
    "objectID": "models.html#modelo-ensemble",
    "href": "models.html#modelo-ensemble",
    "title": "1 Modelagem e previsão",
    "section": "1.7 Modelo ensemble",
    "text": "1.7 Modelo ensemble\nMostramos na seção anterior que temos diferentes modelos preditivos com diferentes features e abordagens. Neste caso, ao invés de avaliar separadamente cada modelo, poder ser interessante combiná-los em um único modelo.\nA técnica de ensemble combina então as previsões de diferentes modelos para melhorar a acurácia. As duas técnicas mais comuns são: - Averaging (Média/Mediana): Combina as previsões de vários modelos, geralmente de forma simples, como a média ponderada. - Stacking (Empilhamento): Combina as previsões de vários modelos através de um meta-modelo. O meta-modelo aprende a combinar as previsões de forma mais complexa, levando em conta as correlações entre elas.\nAbaixo aplicaremos a técnica da mediana do conjunto de modelos, uma vez que esta medida de tendência central é menos sensível à existência de outliers. Outro ponto é que escolhemos quatro modelos para o ensemble, com base no seu desempenho avaliado até então: Prophet, Prophet Boost, LM e GLMNET.\n\n\n\n\n\n\nModelos para ensemble\n\n\nModelos\n\n\n\n\nPROPHET\n\n\nPROPHET W/ XGBOOST ERRORS\n\n\nLM\n\n\nGLMNET\n\n\n\n\n\n\n\n\nTable 6: Modelos para ensemble\n\n\n\n\nO gráfico da Figure 5 mostra que o modelo aparentemente possui uma boa performance, inclusive ao captar os padrões de tendência e sazonalidade da série original.\n\n\n\n\n\n\n\n\nFigure 5: Modelo ensemble - previsão x teste\n\n\n\n\nA Table 7 mostra que o modelo ensemble, de fato, apresenta uma acurácia maior que a de qualquer dos modelos tomados individualmente.\n\n\n\n\n\n\n\n\n\n\n\nMétricas de acurácia do modelo ensemble\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nENSEMBLE (MEDIAN): 4 MODELS\nTest\n5.7\n5.15\n8.2\n5.05\n7.41\n0.16\n\n\n\n\n\n\n\n\nTable 7: Métricas de acurácia do modelo ensemble\n\n\n\n\n\n1.7.1 Refit para o dataset completo e previsão fora-da-amostra\nA seguir fazemos o refit para o modelo ensemble, da mesma forma que foi feito para os modelos individualmente.\n\n\n\n\n\n\n\n\nFigure 6: Previsões para 20 dias fora da amostra\n\n\n\n\nInteressante notarmos que, ao contrário do ocorrido para os valores de teste, na previsão completamente fora do amostra inicial, o modelo não parece performar bem."
  },
  {
    "objectID": "models.html#modelos-utilizando-automlh2o",
    "href": "models.html#modelos-utilizando-automlh2o",
    "title": "1 Modelagem e previsão",
    "section": "1.8 Modelos utilizando AutoML(H2O)",
    "text": "1.8 Modelos utilizando AutoML(H2O)\nTodos os passos anteriormente descritos foram novamente aplicados, porém utilizando o H2O AUTOML.\nO H2O AutoML para previsão é implementado via automl_reg(). Esta função treina e faz validação cruzada de vários modelos de aprendizado de máquina (XGBoost GBM, GLMs, Random Forest, GBMs…) e, em seguida, treina dois modelos Stacked Ensembled, um de todos os modelos e um dos melhores modelos de cada tipo . Finalmente, o melhor modelo é selecionado com base em uma métrica de parada.\nOs melhores modelos estão na Table 8.\n\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         1 hours 11 minutes \n    H2O cluster timezone:       America/Sao_Paulo \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    2 months and 20 days \n    H2O cluster name:           H2O_started_from_R_carlo_ska449 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   7.75 GB \n    H2O cluster total cores:    16 \n    H2O cluster allowed cores:  6 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.3 (2024-02-29 ucrt) \n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n                        model_id        mae       rmse         mse      rmsle\n1 GBM_4_AutoML_8_20240311_232242  0.5469351  0.7364108   0.5423009 0.01050650\n2 DRF_1_AutoML_8_20240311_232242  0.5546981  0.8137777   0.6622341 0.01202878\n3 GBM_3_AutoML_8_20240311_232242  0.5690212  0.7588869   0.5759093 0.01086789\n4 GBM_2_AutoML_8_20240311_232242  0.5885765  0.7857932   0.6174710 0.01129913\n5 GBM_1_AutoML_8_20240311_232242  0.7879059  1.0266900   1.0540924 0.01478201\n6 GLM_1_AutoML_8_20240311_232242 12.7633542 15.0391229 226.1752168 0.22655136\n  mean_residual_deviance\n1              0.5423009\n2              0.6622341\n3              0.5759093\n4              0.6174710\n5              1.0540924\n6            226.1752168\n\n[6 rows x 6 columns] \n\n\n\n\n\n\nMelhores modelos segundo o AutoML\n\n\nmodel_id\nmae\nrmse\nmse\nrmsle\nmean_residual_deviance\n\n\n\n\nGBM_4_AutoML_8_20240311_232242\n0.55\n0.74\n0.54\n0.01\n0.54\n\n\nDRF_1_AutoML_8_20240311_232242\n0.55\n0.81\n0.66\n0.01\n0.66\n\n\nGBM_3_AutoML_8_20240311_232242\n0.57\n0.76\n0.58\n0.01\n0.58\n\n\nGBM_2_AutoML_8_20240311_232242\n0.59\n0.79\n0.62\n0.01\n0.62\n\n\nGBM_1_AutoML_8_20240311_232242\n0.79\n1.03\n1.05\n0.01\n1.05\n\n\nGLM_1_AutoML_8_20240311_232242\n12.76\n15.04\n226.18\n0.23\n226.18\n\n\n\n\n\n\n\n\nTable 8: Previsões para 20 dias fora da amostra\n\n\n\n\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\n\n\n\nMétricas de acurácia do modelo AutoML\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nH2O AUTOML - GBM\nTest\n7.8\n6.9\n11.23\n6.94\n9.92\n0.07\n\n\n\n\n\n\n\n\nTable 9: Métricas de acurácia do modelo AutoML\n\n\n\n\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\nFigure 7: Modelo AutoML - previsão x teste\n\n\n\n\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n                        model_id        mae       rmse         mse      rmsle\n1 DRF_1_AutoML_9_20240311_232506  0.5872533  0.8125059   0.6601659 0.01058995\n2 GBM_3_AutoML_9_20240311_232506  0.6129993  0.8162672   0.6662921 0.01088953\n3 GBM_2_AutoML_9_20240311_232506  0.6376214  0.8437928   0.7119863 0.01127391\n4 GBM_1_AutoML_9_20240311_232506  0.8433915  1.0877047   1.1831016 0.01450204\n5 GLM_1_AutoML_9_20240311_232506 11.7284783 14.2006439 201.6582884 0.21667184\n  mean_residual_deviance\n1              0.6601659\n2              0.6662921\n3              0.7119863\n4              1.1831016\n5            201.6582884\n\n[5 rows x 6 columns] \n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\nFigure 8: Previsões para 20 dias fora da amostra\n\n\n\n\n\n1.8.1 Comparação entre previsto e real\nNa Table 10 temos as métricas de acurácia para cada um dos modelos, comparando as previsões fora da amostra com os valores efetivamente observados de 01/02/2024 a 23/02/2024. Os valores são comparados estritamente com os dias em que o pregão ocorreu.\nSurpreendentemente, o modelo ARIMA(0,1,0) W/ XGBOOST ERRORS é aquele com o menor MAPE. Por outro lado, os modelos do tipo PROPHET e ENSEMBLE parecem ser os mais equilibrados, pois possuem um MAPE pequeno, ao mesmo tempo em que sabemos a capacidade que eles possuem em captar os padrões de tendência e sazonalidade.\n\n\n\n\n\n\nMétricas de acurácia - previsão (fora da amostra) x valores reais\n\n\nModelo\nMAE\nMAPE\nMASE\nSMAPE\nRMSE\n\n\n\n\nARIMA(0,1,0) W/ XGBOOST ERRORS\n0.90\n0.69\n0.91\n0.70\n1.12\n\n\nARIMA(0,1,0)\n1.02\n0.79\n1.04\n0.80\n1.32\n\n\nETS(M,AD,N)\n1.14\n0.88\n1.16\n0.89\n1.45\n\n\nPROPHET W/ XGBOOST ERRORS\n1.93\n1.50\n1.96\n1.51\n2.22\n\n\nPROPHET\n2.53\n1.96\n2.57\n1.98\n2.75\n\n\nENSEMBLE (MEDIAN): 4 MODELS\n3.63\n2.82\n3.68\n2.86\n3.78\n\n\nGLMNET\n4.72\n3.67\n4.80\n3.74\n4.84\n\n\nLM\n11.71\n9.10\n11.89\n9.54\n11.78\n\n\nUPDATE: H2O AUTOML - DRF\n12.65\n9.83\n12.85\n10.35\n12.79\n\n\n\n\n\n\n\n\nTable 10: Métricas de acurácia - previsão x dados reais fora da amostra"
  },
  {
    "objectID": "eda.html#visualização-e-regularização",
    "href": "eda.html#visualização-e-regularização",
    "title": "1 Análise exploratória e visualização",
    "section": "1.1 Visualização e regularização",
    "text": "1.1 Visualização e regularização\n\nPara realizar a modelagem da série temporal do Ibovespa, será necessário realizar algumas análises anteriores, incluindo certos ajustes e eliminação de ruídos que permitam, ao mesmo tempo, um ajuste mais fácil e a manutenção das principais características da série para se ter uma predição mais acurada.\nA visualização da ?@fig-dados_regularizados é da série de fechamento com dados até 01/02/2024. Este pequeno corte da série tem por objetivo, ao final do processo de modelagem, avaliar a capacidade preditiva do modelo “fora-da-amostra”, mas com a vantagem de se poder comparar as previsões com dados reais.\nA série temporal também foi regularizada, uma vez que esta possui dados somente para os dias em que o pregão ocorreu (dias úteis). Como os modelos de séries temporais lidam melhor com dados regulares, foi aplicado um algoritmo de regularização para preencher as lacunas existentes.\n\n\nCode\n#|echo: false\n#|message: false\n#|warning: false\n#|label: fig-dados_regularizados\n#|fig-cap: 'Ibovespa - jan/2010 a fev/2024'\n\nclose_df &lt;- ibovespa |&gt; select(c(date, close)) |&gt; filter(date &lt; '2024-02-01')\n\nclose_reg &lt;- close_df |&gt; \n    pad_by_time(date, .by = \"auto\") |&gt; \n    mutate_at(vars(close), .funs = ts_impute_vec, period = 1)\n\n\npad applied on the interval: day\n\n\nCode\nclose_reg |&gt; plot_time_series(date, close, .interactive = T, .plotly_slider = T, .title = 'Ibovespa regularizado')\n\n\n\n\n\n\nO resultado também está na tabela ?@tbl-dados_regularizados.\n\n\nCode\n#|echo: false\n#|message: false\n#|warning: false\n#|label: tbl-dados_regularizados\n#|tbl-cap: \"\"\n#|\n# regularização para evitar descontinuidades por dias sem pregão\n\n\n# imprime a tabela de dados\nDT::datatable(close_reg, filter = \"bottom\", \n                          colnames = c(\"Data\",\"Fechamento\"),\n                          # caption = header, \n                          extensions = 'Buttons', \n                          options = list(\n                              dom = 'Bfrtip',\n                              buttons = c('copy','excel', 'csv', 'pdf')\n                          )) |&gt;\n                DT::formatDate(columns = c('date'), 'toLocaleDateString') |&gt;\n                DT::formatRound(columns = c('close'),\n                                mark = \".\",\n                                dec.mark = \",\")"
  },
  {
    "objectID": "eda.html#caracterização-do-processo-gerador-de-dados-da-série-temporal",
    "href": "eda.html#caracterização-do-processo-gerador-de-dados-da-série-temporal",
    "title": "1 Análise exploratória e visualização",
    "section": "1.2 Caracterização do processo gerador de dados da série-temporal",
    "text": "1.2 Caracterização do processo gerador de dados da série-temporal\n\n1.2.1 Estacionariedade e decomposição\n\nUm premissa fundamental da nossa modelagem é que a série será tratada como um processo univariado, ou seja, as suas características de interesse serão explicadas somente por ela mesma.\nNeste sentido um bom primeiro passo para identificar quais seriam os modelos mais adequados, é determinar se a série é ou não estacionária.\n\n1.2.1.1 Testes de raiz unitária\n\nA estacionariedade é uma propriedade importante na análise de séries temporais. Um grande número de modelos assumem a estacionariedade do processo (como os modelos ARMA e ARIMA) e, além disso,um modelo de série temporal que não é estacionário irá variar a sua acurácia à medida que as métricas da série de tempo variarem1.\nAssim, na análise de séries temporais é possível se utilizar de estratégias como a transformação logarítimica, a transformação quadrática ou ainda a diferenciação. Vale dizer que as duas primeiras buscam atacar a alteração da variância no tempo, enquanto que a última foca na remoção da tendência.\nPara identificar a existência de raiz unitária, isto é, não estacionariedade, os testes que utilizaremos são: - Augmented Dickey-Fuller (ADF) - Hipótese nula (\\(H_0\\)): a série possui uma raiz unitária, logo não é estacionária2; - Kwiatkowski–Phillips–Schmidt–Shin (KPSS) - Hipótese nula (\\(H_0\\)): a série não possui uma raiz unitária, logo é estacionária3.\nO resultado a seguir se referece à aplicação dos testes ADF e KPSS à série.\n\n\nCode\n#|echo: false\n#|message: false\n#|warning: false\n\nclose_ts &lt;- ts(close_reg$close, start = c(2010,01,04), end = c(2024,02,01), frequency = 365)\n\n\n\n\n\n\n\n\n\nResultado do teste:\n\n\n\n\n\nCode\nur_test(close_ts, type_adf = \"trend\", type_kpss = \"tau\")\n\n\n############################\n    \nResultado do Teste ADF\n    \n############################\n\n    \np-valor: 0.878\n\nEstatísticas do teste ADF (5% de significância):\n\n           tau3     phi2     phi3\ncvals -3.410000 4.680000 6.250000\nstats -2.631176 3.020795 4.118092\n\n\n############################\n    \nResultado do Teste KPSS\n    \n############################\n\n    \nValor crítico a 5% de significância: 0.146\n\nEstatística do teste KPSS: 7.478\n\n\n\n\nNo teste ADF o p-valor mostra que a hipótese \\(H_0\\) não pode ser rejeitada considerando 5% de nível de significância - 0,878 &gt; 0,05 e o módulo das estatísticas \\(\\tau_3\\), \\(\\phi_2\\) e \\(\\phi_3\\) é menor que o módulo dos valores críticos. No teste KPSS o resultado indica que a hipótese \\(H_0\\) pode ser rejeitada a 5% de significância, visto que 7,582 &gt; 0,146.\nOs testes indicam, portanto, que a série não é estacionária.\n\n\n\n\n1.2.2 Decomposição e testes de estacionariedade\n\n\n1.2.3 Funções de autocorrelação (ACF) e autorrelação parcial (PACF)\n\nA função de autocorrelação (ACF) e a função de autocorrelação parcial (PACF) são medidas de associação entre valores autais e valores pregressos em séries temporais[^exploratory_yearly-1]. Portanto, indicam em que medida um valor \\(x_t\\) é dependente do valor \\(x_{t-1}\\) e, consequentemente, o passado é últil para prever o futuro.\nA autocorrelação parcial é mais útil durante o processo de especificação de um modelo autoregressivo. Ela ajuda a avaliar as propriedades de uma série temporal.\nAs funções de autocorrelação e autocorrelação parcial também servem para estudar a estacionariedade de uma série temporal[^exploratory_yearly-2].Uma série temporal estacionária tem funções de média, variância e autocorrelação que são essencialmente constantes ao longo do tempo[^exploratory_yearly-3]. A função de autocorrelação diminui para quase zero rapidamente para uma série temporal estacionária (decaimento exponencial).\nOs modelos matemáticos mais comuns e que têm como premissa apresentar a estacionariedade são modelos auto-regressivos - AR (p), auto-regressivo e de média móvel - ARMA (p,q) - e modelo auto—regressivo integrado e de média móvel - ARIMA(p,d,q)[^exploratory_yearly-4].\nPara uma série temporal estacionária, um modelo de média móvel vê o valor de uma variável no tempo \\(t\\) como uma função linear de erros residuais de \\(q\\) defasagens. Já um processo auto-regressivo de ordem \\(p\\) é um modelo que utiliza os próprios valores passados como preditores[^exploratory_yearly-5]. O termo \\(d\\) especifica a ordem de integração da série (ou seja, quantas vezes a série deve ser diferenciada para se tornar estacionária).\nAs ordens dos processos em termos de \\(p\\) e \\(q\\) são definidas com base na análise das funções de autocorrelação e autocorrelação parcial. A ordem \\(p\\) do processo auto-regressivo é determinada pela função de autocorrelação parcial, enquanto a ordem \\(q\\) do processo de média móvel é indicada pelo número de correlações estatisticamente significativas na função de autocorrelação[^exploratory_yearly-6].\nEstabelecidos os conceitos e aplicações da ACF e da PACF, passemos à análise das séries de interesse.\n\n\nCode\n#|echo: false\n#|message: false\n#|warning: false\n#|label: fig-acf_pacf\n#|fig-cap: 'ACF e PACF'\n\n\nts_cor(close_ts, seasonal = T, lag.max = 30, seasonal_lags = 15)\n\n\n\n\n\n\nA ACF mostra que existe uma autocorrelação persistente entre as defasagens da série temporal, a qual pode ser melhor visualizada no gráfico da ?@fig-acf para 7, 30, 60 e 90 defasagens.Com 60 defasagens a correlação já é menos forte. Também há a autocorrelação sazonal para 15 lags, a qual acompanha o comportamento da ACF. A PACF, por seu turno, indica que existe um processo do tipo AR(1), pois há um pico no primeiro lag.\n\n\nCode\n#|echo: false\n#|message: false\n#|warning: false\n#|label: fig-acf\n#|fig-cap: 'Relacionamento linear entre os lags 7, 30, 60 e 90'\n\n# relacionamento linear entre os lags 7, 30, 60 e 90 dias\nts_lags(close_ts, lags = c(7, 30, 60, 90))\n\n\n\n\n\n\n\n\n\n1.2.4 Decomposição da série, detecção de anomalias e changepoints\n\nA existência de quebras estruturais4 nas séries temporais além de serem eventual causa de existência de raiz unitária (não estacionariedade), também podem auxiliar a compreensão do fenômeno analisado, vez que indicam a existência, por exemplo, de alterações na tendência da série e, deste modo, podem ajudar a corroborar hipóteses levantadas por quem está realizando a análise.\nO primeiro passo para a detecção de anomalias é a decomposição da série em suas componentes de tendência e sazonalidade, além de podermos também verificar os resíduos das séries.\nOs gráficos da Figure 1 mostra que existe um forte componente sazonal atuando e a tendência é crescente.\n\n\n\n\n\n\n\n\nFigure 1: Decomposição da série temporal\n\n\n\n\nOs gráficos da Figure 1 permitem identicar a existência de um componente sazonal importante na série, bem como resíduos relevantes por volta de 2020, mostra a magnitude do choque causado pela pandemia de Covid-19.\nA Figure 2 mostra também mostra uma grande quantidade de anomalias identificadas no primeiro semestre de 2020, novamente um observação justificada pela pandemia, mais especificamente pelos lockdowns.\n\n\n\n\n\n\n\n\nFigure 2: Detecção de anomalias - Ibovespa\n\n\n\n\nOs dias em que foram identificadas as anomalidas estão na tabela Table 1.\n\n\n\n\n\n\n\n\n\n\n\n\nDatas em que foram identificadas as anomalias\n\n\n\nData\n\n\n\n\n2010-02-05\n\n\n2010-05-20\n\n\n2011-08-06\n\n\n2011-08-07\n\n\n2011-08-08\n\n\n2011-08-09\n\n\n2011-08-10\n\n\n2018-05-16\n\n\n2018-06-14\n\n\n2018-06-15\n\n\n2018-06-16\n\n\n2018-06-17\n\n\n2018-06-18\n\n\n2018-06-21\n\n\n2018-06-22\n\n\n2018-06-23\n\n\n2018-06-24\n\n\n2018-06-27\n\n\n2018-12-25\n\n\n2018-12-26\n\n\n2018-12-27\n\n\n2019-05-16\n\n\n2019-05-17\n\n\n2019-05-18\n\n\n2019-08-24\n\n\n2019-08-25\n\n\n2019-08-26\n\n\n2019-08-27\n\n\n2019-10-08\n\n\n2020-02-12\n\n\n2020-02-17\n\n\n2020-02-18\n\n\n2020-02-19\n\n\n2020-02-20\n\n\n2020-03-07\n\n\n2020-03-08\n\n\n2020-03-09\n\n\n2020-03-10\n\n\n2020-03-11\n\n\n2020-03-12\n\n\n2020-03-13\n\n\n2020-03-14\n\n\n2020-03-15\n\n\n2020-03-16\n\n\n2020-03-17\n\n\n2020-03-18\n\n\n2020-03-19\n\n\n2020-03-20\n\n\n2020-03-21\n\n\n2020-03-22\n\n\n2020-03-23\n\n\n2020-03-24\n\n\n2020-03-25\n\n\n2020-03-26\n\n\n2020-03-27\n\n\n2020-03-28\n\n\n2020-03-29\n\n\n2020-03-30\n\n\n2020-03-31\n\n\n2020-04-01\n\n\n2020-04-02\n\n\n2020-04-03\n\n\n2020-04-04\n\n\n2020-04-05\n\n\n2020-04-06\n\n\n2020-04-07\n\n\n2020-05-12\n\n\n2020-05-13\n\n\n2020-05-15\n\n\n2020-05-16\n\n\n2020-06-05\n\n\n2020-06-06\n\n\n2020-06-07\n\n\n2020-06-08\n\n\n2020-06-09\n\n\n2020-09-29\n\n\n2020-10-28\n\n\n2020-10-29\n\n\n2020-10-30\n\n\n2020-10-31\n\n\n2020-11-01\n\n\n2020-11-02\n\n\n2020-11-03\n\n\n2020-11-04\n\n\n2021-01-08\n\n\n2021-01-09\n\n\n2021-01-10\n\n\n2021-01-11\n\n\n2021-01-12\n\n\n2021-01-14\n\n\n2021-02-26\n\n\n2021-02-27\n\n\n2021-02-28\n\n\n2021-03-01\n\n\n2021-03-02\n\n\n2021-03-03\n\n\n2021-03-08\n\n\n2021-03-09\n\n\n2021-09-20\n\n\n2021-10-15\n\n\n2021-10-16\n\n\n2021-10-17\n\n\n2021-10-18\n\n\n2022-01-05\n\n\n2022-01-06\n\n\n2022-01-08\n\n\n2022-01-09\n\n\n2022-01-10\n\n\n2022-03-15\n\n\n2022-03-24\n\n\n2022-03-25\n\n\n2022-03-26\n\n\n2022-03-27\n\n\n2022-03-28\n\n\n2022-03-29\n\n\n2022-03-30\n\n\n2022-03-31\n\n\n2022-04-01\n\n\n2022-04-02\n\n\n2022-04-03\n\n\n2022-04-04\n\n\n2022-04-05\n\n\n2022-04-06\n\n\n2022-04-07\n\n\n2022-04-08\n\n\n2022-04-09\n\n\n2022-04-10\n\n\n2022-04-11\n\n\n2022-05-09\n\n\n2022-05-10\n\n\n2022-05-24\n\n\n2022-05-25\n\n\n2022-05-26\n\n\n2022-05-27\n\n\n2022-05-28\n\n\n2022-05-29\n\n\n2022-05-30\n\n\n2022-05-31\n\n\n2022-06-01\n\n\n2022-06-02\n\n\n2022-06-03\n\n\n2022-06-04\n\n\n2022-06-05\n\n\n2022-06-06\n\n\n2022-06-07\n\n\n2022-06-08\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-17\n\n\n2022-07-18\n\n\n2022-08-12\n\n\n2022-08-13\n\n\n2022-08-14\n\n\n2022-08-15\n\n\n2022-08-16\n\n\n2022-08-17\n\n\n2022-08-18\n\n\n2022-09-29\n\n\n2022-10-21\n\n\n2022-10-22\n\n\n2022-11-04\n\n\n2022-12-13\n\n\n2022-12-14\n\n\n2022-12-15\n\n\n2022-12-16\n\n\n2022-12-17\n\n\n2022-12-18\n\n\n2023-01-03\n\n\n2023-03-23\n\n\n2023-10-22\n\n\n2023-10-23\n\n\n2023-10-25\n\n\n2023-10-27\n\n\n2023-10-28\n\n\n2023-10-29\n\n\n2023-10-30\n\n\n2023-10-31\n\n\n2023-12-27\n\n\n2023-12-28\n\n\n\n\n\n\n\n\nTable 1: Datas em que foram identificadas as anomalias.\n\n\n\n\nPodemos também comparar as datas em que foram identificadas as anomalias pelo algoritmo anterior, com os “pontos de quebra” (ou de mudança) identificados pelo modelo Prophet.\n\n\n\n\n\n\n\n\nFigure 3: Changepoints (pontos de quebra) identificados via Prophet\n\n\n\n\n\nOs dias em que foram identificados changepoints estão na tabela Table 2.\n\n\n\n\n\n\n\n\n\n\n\n\nDatas em que foram identificados os `changepoints`\n\n\n\nData\n\n\n\n\n2010-07-18\n\n\n2011-01-30\n\n\n2011-08-13\n\n\n2012-02-24\n\n\n2012-09-06\n\n\n2013-03-21\n\n\n2013-10-02\n\n\n2014-04-15\n\n\n2014-10-28\n\n\n2015-05-11\n\n\n2015-11-22\n\n\n2016-06-04\n\n\n2016-12-17\n\n\n2017-06-30\n\n\n2018-01-11\n\n\n2018-07-25\n\n\n2019-02-06\n\n\n2019-08-20\n\n\n2020-03-02\n\n\n2020-09-14\n\n\n2021-03-28\n\n\n2021-10-09\n\n\n2022-04-22\n\n\n2022-11-04\n\n\n2023-05-18\n\n\n\n\n\n\n\n\nTable 2: Datas em que foram identificados os changepoints.\n\n\n\n\nUma forma de melhor visualizar o período mais crítico para a modelagem, ou seja, o período em que existe maior concentração de volatilidade e mudanças de tendência, é por meio do histograma a seguir.\n\n\n\n\n\n\n\n\nFigure 4: Histograma de changepoints e anomalias.\n\n\n\n\nAs anomalias/changepoints estão relacionados essencialmente concentrados 2020 e 2022. Em 2020 tivemos a pandemia de Covid-19, enquanto que em 2022 tivemos eleições gerais, inclusive para Presidente da República. Tais eventos podem ajudar a explicar a grande quantidade de dados considerados anômalos.\n\n\n\n\n\n\n\n\nFigure 5: Série temporal com dados observados e após eliminação de anomalias\n\n\n\n\nTendo em vista que a série “limpa” possui menor quantidade de valores extremos, esta será, portanto, aquela que tomaremos como base para realizar a nossa modelagem e previsão."
  },
  {
    "objectID": "eda.html#footnotes",
    "href": "eda.html#footnotes",
    "title": "1 Análise exploratória e visualização",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNielsen, Aileen.Practical Time Series Analysis: Prediction with Statistics and Machine Learning.O’Reilly Media; 1ª edição (19 novembro 2019).pp.85↩︎\nhttps://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test↩︎\nhttps://en.wikipedia.org/wiki/KPSS_test↩︎\nhttps://en.wikipedia.org/wiki/Structural_break#:~:text=Structural%20break%20tests,-A%20single%20break&text=For%20linear%20regression%20models%2C%20the,…%2CT%5D.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "web_tech02",
    "section": "",
    "text": "1 Introdução\n\nDescrição do Problema:\nDesenvolver um modelo preditivo com os dados da Ibovespa para criar uma série temporal e prever diariamente o índice de fechamento da base.\nBase de dados:\nÍndices diários da Ibovespa disponíveis no sítio da investing, de janeiro de 2010 a fevereiro de 2023.\nRequisitos: \n\nModelo com storytelling, desde a captura do dado até a entrega do modelo;\nJustificar a(s) técnica(s) utilizada(s);\nAtingir um nível de acurácia acima de 70%.\n\nNa tabela ?@tbl-dados_completos é possível visualizar os dados extraídos do site da investing, de janeiro de 2010 a fevereiro de 2023.\n\n#|echo: false\n#|message: false\n#|warning: false\n#|label: tbl-dados_completos\n#|tbl-cap: \"\"\n\n\n# leitura do arquivo de dados  \nibovespa &lt;- read_csv(\"C:/projects/alura_postech/fiap_techchallenge02/data/Ibovespa_20100101-20240224.csv\") |&gt; \n    mutate(Data = as.Date(Data, format = \"%d.%m.%Y\"))\n\nRows: 3503 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Data, Vol., Var%\ndbl (4): Último, Abertura, Máxima, Mínima\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# renomeia as colunas\ncolnames(ibovespa) &lt;- c('date', 'close', 'open', 'max', 'min','vol', 'var')\n\n# imprime a tabela de dados\nDT::datatable(ibovespa, filter = \"bottom\", \n                          colnames = c('Data', 'Fechamento', 'Abertura', \n                                                'Máx.', 'Min.', 'Vol.', 'Var.(%)'),\n                          caption = \"Ibovespa - jan/2010 a fev/2024\", \n                          extensions = 'Buttons', \n                          options = list(\n                              dom = 'Bfrtip',\n                              buttons = c('copy','excel', 'csv', 'pdf')\n                          )) |&gt;\n                DT::formatDate(columns = c('date'), 'toLocaleDateString') |&gt;\n                DT::formatRound(columns = c('close', 'open', 'max', 'min'),\n                                mark = \".\",\n                                dec.mark = \",\") \n\n\n\n\n\nConforme estabelecido nos requisitos, o objetivo é desenvolver um modelo preditivo para prever diariamente índice de fechamento da base. Desta forma, estamos interessados somente na coluna relativa ao índice de fechamento, cujos dados podem ser visualizados no gráfico da ?@fig-dados_completos.\n\n#|echo: false\n#|message: false\n#|warning: false\n#|label: fig-dados_completos\n#|fig-cap: 'Ibovespa - jan/2010 a fev/2024'\n# série temporal completa\nibovespa |&gt; plot_time_series(date, close,\n                             .interactive = T, \n                             .plotly_slider = T, \n                             .title = \"Ibovespa - Fechamento\",\n                             .smooth = 0.05)\n\n\n\n\nsave(list=c('ibovespa'), file='index.out.RData')\n\nA série temporal em questão possui 3503 linhas. No início do período analisado o índice era da ordem de 70 pontos, e o último dado, de 23/02/2024, é de 129 pontos. Ou seja, o índice cresceu 84% em um período de 5163 dias (pouco mais de 14 anos).\nVários são os fatores que influenciam o comportamento do mercado, como: - Desempenho das empresas: O lucro, a receita e o crescimento das empresas são fatores importantes que influenciam o preço de suas ações. - Condições econômicas: O crescimento do PIB, a taxa de inflação e a taxa de juros são alguns indicadores que afetam o mercado de ações. - Sentimento do mercado: A confiança dos investidores e a percepção do risco também influenciam o mercado. - Choques externos: Guerras, epidemias e pandemias.\nPor meio de inspeção visual é possível notar que a série possui uma tendência de alta a partir de 2016, refletindo um certo otimismo à época, pois o Brasil estava saindo da crise econômica na qual estava imerso desde 2012. Entre 2020 e 2021 a Pandemia de Covid-19, com seus lockdowns e outras medidas adotadas para mitigar a contaminação, naturalmente causou um impacto considerável, evidenciado pela queda ocorrida entre 23/01/2020 e 23/02/2020, da ordem de 50%.\nPara além dos choques exógenos, séries financeiras são bastante desafiadoras para aqueles que pretendem construir modelos preditivos. Especialmente em séries diárias e de grande volatilidade, existe bastante ruído e efeitos sazonais, os quais impactam substancialmente a capacidade preditiva da grande maioria dos modelos.\nA não linearidade também é uma característica da série temporal em questão, onde a resposta do mercado a diferentes tipos de choques (grandes ou pequenos, positivos ou negativos) pode variar, contribuindo para a aparência de um passeio aleatório, ou seja, completamente imprevisível.\nAlém disso, séries temporais desse tipo também podem ser compreendidas como um passeio aleatório devido à hipótese do mercado eficiente. Essa hipótese sugere que os preços dos ativos financeiros refletem todas as informações disponíveis e, portanto, qualquer mudança nos preços é resultado de novas informações que são imprevisíveis.\nOutrossim, como já apontado anteriormente, séries temporais como o Ibovespa são influenciadas por uma infinidade de fatores, como decisões econômicas, políticas, eventos globais e até mesmo o comportamento humano, que são complexos e muitas vezes aleatórios. Isso faz com que a sua trajetória seja imprevisível e possa parecer um passeio aleatório, onde os movimentos futuros não podem ser previstos com base nos movimentos passados.\nEmbora o mercado de ações possa se assemelhar a um passeio aleatório no curto prazo, existem tendências e fatores que podem ser analisados para fazer previsões mais precisas no longo prazo. A dificuldade de prever o mercado se dá pela complexa interação de diversos fatores, tanto previsíveis quanto imprevisíveis.\nPor isso que o trabalho de modelar uma série como a do índice Ibovespa deve ser precedido de uma análise exploratória que permita compreender melhor a série e seus principais componentes com o objetivo de escolher o modelo e os parâmetros mais adequados para o melhor ajuste da série."
  }
]